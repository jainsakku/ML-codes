# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MVJ8InMTPacr8vA7t-c1TtxAhXJGnJdy
"""

import logging
import pandas as pd
import numpy as np
from numpy import random
import gensim
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import re
from bs4 import BeautifulSoup

url="https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv"
df = pd.read_csv(url)
df = df[pd.notnull(df['tags'])]

print(df.head(10))
print(df['post'].apply(lambda x: len(x.split(' '))).sum())

def print_plot(index):
    example = df[df.index == index][['post', 'tags']].values[0]
    if len(example) > 0:
        print(example[0])
        print('Tag:', example[1])

print_plot(10)

nltk.download('stopwords')

import nltk

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
        text: a string
        
        return: modified initial string
    """
    text = BeautifulSoup(text, "lxml").text # HTML decoding
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    return text
    
df['post'] = df['post'].apply(clean_text)
print_plot(10)

from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics import classification_report

from nltk.stem import WordNetLemmatizer
from nltk import tokenize
import re
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn import metrics
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.naive_bayes import GaussianNB
from sklearn import svm
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
X = df['post']
y = df.tags
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)

vectorizer = TfidfVectorizer(stop_words='english',
max_features= 200,
max_df = 0.5, 
smooth_idf=True)


tf=vectorizer.fit_transform(X_train)
test=vectorizer.transform(X_test)

from sklearn.decomposition import PCA

pca = PCA(n_components=30)
tf1 =pca.fit_transform(tf.toarray())
test1=pca.transform(test.toarray())

clf = svm.SVC(gamma='scale')
clf.fit(tf1, y_train)  
y_pred=clf.predict(test1)


print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred))