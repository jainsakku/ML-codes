# -*- coding: utf-8 -*-
"""SVD_SVM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IhPkT0k2kOZP_HvXFbsFdmIAlV2zA49J
"""

import logging
import pandas as pd
import numpy as np
from numpy import random
import gensim
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import re
from bs4 import BeautifulSoup

url="https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv"
df = pd.read_csv(url)
df = df[pd.notnull(df['tags'])]

print(df.head(10))
print(df['post'].apply(lambda x: len(x.split(' '))).sum())

def print_plot(index):
    example = df[df.index == index][['post', 'tags']].values[0]
    if len(example) > 0:
        print(example[0])
        print('Tag:', example[1])

print_plot(10)

nltk.download('stopwords')

import nltk

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
        text: a string
        
        return: modified initial string
    """
    text = BeautifulSoup(text, "lxml").text # HTML decoding
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    return text
    
df['post'] = df['post'].apply(clean_text)
print(df['post'].apply(lambda x: len(x.split(' '))).sum())
print_plot(10)

from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics import classification_report

from nltk.stem import WordNetLemmatizer
from nltk import tokenize
import re
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn import metrics
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.naive_bayes import GaussianNB
from sklearn import svm
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
X = df['post']
y = df.tags
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)
#count_vect = CountVectorizer(binary=False,ngram_range=(1,1))

length =342429
m = np.zeros([length,length]) # n is the count of all words
def cal_occ(sentence,m):
    for i,word in enumerate(sentence):
        for j in range(max(i-window,0),min(i+window,length)):
             m[word,sentence[j]]+=1
for sentence in X:
    cal_occ(sentence, m)

vectorizer = TfidfVectorizer(stop_words='english',
max_features= 20000,
max_df = 0.5, 
smooth_idf=True)


tf=vectorizer.fit_transform(X_train)
test=vectorizer.transform(X_test)

svd_model = TruncatedSVD(n_components=300, algorithm='randomized', n_iter=100, random_state=42)

tf1=svd_model.fit_transform(tf)

test1=svd_model.transform(test)

clf = svm.SVC(gamma='scale')
clf.fit(tf1, y_train)  
y_pred=clf.predict(test1)


print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression() 


clf.fit(tf1, y_train)  
y_pred=clf.predict(test1)



print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred))

from google.colab import drive
drive.mount('/content/drive')

cp /content/drive/My\ Drive/GoogleNews-vectors-negative300.bin.gz /content

pip install gensim

import gensim
from gensim.models import Word2Vec

wv = gensim.models.KeyedVectors.load_word2vec_format("GoogleNews-vectors-negative300.bin.gz", binary=True)
wv.init_sims(replace=True)

priny